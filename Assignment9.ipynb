{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ],
      "metadata": {
        "id": "UWiTyAhw4Z6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords', force=True)\n",
        "# Download the 'punkt_tab' resource for sentence tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Original paragraph\n",
        "paragraph = \"\"\"I really like technology because it changes how we live and work.\n",
        "               New things like smartwatches, robots, and AI make life easier and more fun.\n",
        "               It helps people talk and share ideas from anywhere in the world.\n",
        "               I enjoy seeing how fast new inventions come out.\n",
        "               Technology also makes us think about how we use it wisely.\n",
        "               It’s exciting to see what’s coming next.\"\"\"\n",
        "\n",
        "# Convert to lowercase\n",
        "lower_text = paragraph.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "clean_text = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Tokenize into words and sentences\n",
        "words = word_tokenize(clean_text)\n",
        "sentences = sent_tokenize(clean_text)\n",
        "\n",
        "# Display tokens\n",
        "print(\"Word Tokenization:\\n\", words)\n",
        "print(\"\\nSentence Tokenization:\\n\", sentences)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Display filtered words\n",
        "print(\"\\nFiltered Words (No Stopwords):\\n\", filtered_words)\n",
        "\n",
        "# Word frequency distribution\n",
        "freq_dist = FreqDist(filtered_words)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwPo0NyR59ha",
        "outputId": "2f6baa1a-4338-4c55-8f02-5c6b751e78e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            " ['i', 'really', 'like', 'technology', 'because', 'it', 'changes', 'how', 'we', 'live', 'and', 'work', 'new', 'things', 'like', 'smartwatches', 'robots', 'and', 'ai', 'make', 'life', 'easier', 'and', 'more', 'fun', 'it', 'helps', 'people', 'talk', 'and', 'share', 'ideas', 'from', 'anywhere', 'in', 'the', 'world', 'i', 'enjoy', 'seeing', 'how', 'fast', 'new', 'inventions', 'come', 'out', 'technology', 'also', 'makes', 'us', 'think', 'about', 'how', 'we', 'use', 'it', 'wisely', 'it', '’', 's', 'exciting', 'to', 'see', 'what', '’', 's', 'coming', 'next']\n",
            "\n",
            "Sentence Tokenization:\n",
            " ['i really like technology because it changes how we live and work\\n               new things like smartwatches robots and ai make life easier and more fun\\n               it helps people talk and share ideas from anywhere in the world\\n               i enjoy seeing how fast new inventions come out\\n               technology also makes us think about how we use it wisely\\n               it’s exciting to see what’s coming next']\n",
            "\n",
            "Filtered Words (No Stopwords):\n",
            " ['really', 'like', 'technology', 'changes', 'live', 'work', 'new', 'things', 'like', 'smartwatches', 'robots', 'ai', 'make', 'life', 'easier', 'fun', 'helps', 'people', 'talk', 'share', 'ideas', 'anywhere', 'world', 'enjoy', 'seeing', 'fast', 'new', 'inventions', 'come', 'technology', 'also', 'makes', 'us', 'think', 'use', 'wisely', '’', 'exciting', 'see', '’', 'coming', 'next']\n",
            "\n",
            "Word Frequency Distribution:\n",
            "really: 1\n",
            "like: 2\n",
            "technology: 2\n",
            "changes: 1\n",
            "live: 1\n",
            "work: 1\n",
            "new: 2\n",
            "things: 1\n",
            "smartwatches: 1\n",
            "robots: 1\n",
            "ai: 1\n",
            "make: 1\n",
            "life: 1\n",
            "easier: 1\n",
            "fun: 1\n",
            "helps: 1\n",
            "people: 1\n",
            "talk: 1\n",
            "share: 1\n",
            "ideas: 1\n",
            "anywhere: 1\n",
            "world: 1\n",
            "enjoy: 1\n",
            "seeing: 1\n",
            "fast: 1\n",
            "inventions: 1\n",
            "come: 1\n",
            "also: 1\n",
            "makes: 1\n",
            "us: 1\n",
            "think: 1\n",
            "use: 1\n",
            "wisely: 1\n",
            "’: 2\n",
            "exciting: 1\n",
            "see: 1\n",
            "coming: 1\n",
            "next: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques."
      ],
      "metadata": {
        "id": "WcVDkBkYAEME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download required data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Paragraph\n",
        "paragraph = \"\"\"I really like technology because it changes how we live and work.\n",
        "               New things like smartwatches, robots, and AI make life easier and more fun.\n",
        "               It helps people talk and share ideas from anywhere in the world.\n",
        "               I enjoy seeing how fast new inventions come out.\n",
        "               Technology also makes us think about how we use it wisely.\n",
        "               It’s exciting to see what’s coming next.\"\"\"\n",
        "\n",
        "# Preprocessing\n",
        "lower_text = paragraph.lower()\n",
        "clean_text = lower_text.translate(str.maketrans('', '', string.punctuation))\n",
        "words = word_tokenize(clean_text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Initialize stemmers and lemmatizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply stemming and lemmatization\n",
        "print(f\"{'Word':<15}{'Porter':<15}{'Lancaster':<15}{'Lemma':<15}\")\n",
        "print(\"-\" * 60)\n",
        "for word in filtered_words:\n",
        "    porter_stem = porter.stem(word)\n",
        "    lancaster_stem = lancaster.stem(word)\n",
        "    lemma = lemmatizer.lemmatize(word)  # Default POS is noun\n",
        "    print(f\"{word:<15}{porter_stem:<15}{lancaster_stem:<15}{lemma:<15}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H7dXPecAIEb",
        "outputId": "fbc3c94c-76ff-4df6-a3c8-d9d1fd648d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word           Porter         Lancaster      Lemma          \n",
            "------------------------------------------------------------\n",
            "really         realli         real           really         \n",
            "like           like           lik            like           \n",
            "technology     technolog      technolog      technology     \n",
            "changes        chang          chang          change         \n",
            "live           live           liv            live           \n",
            "work           work           work           work           \n",
            "new            new            new            new            \n",
            "things         thing          thing          thing          \n",
            "like           like           lik            like           \n",
            "smartwatches   smartwatch     smartwatch     smartwatches   \n",
            "robots         robot          robot          robot          \n",
            "ai             ai             ai             ai             \n",
            "make           make           mak            make           \n",
            "life           life           lif            life           \n",
            "easier         easier         easy           easier         \n",
            "fun            fun            fun            fun            \n",
            "helps          help           help           help           \n",
            "people         peopl          peopl          people         \n",
            "talk           talk           talk           talk           \n",
            "share          share          shar           share          \n",
            "ideas          idea           idea           idea           \n",
            "anywhere       anywher        anywh          anywhere       \n",
            "world          world          world          world          \n",
            "enjoy          enjoy          enjoy          enjoy          \n",
            "seeing         see            see            seeing         \n",
            "fast           fast           fast           fast           \n",
            "new            new            new            new            \n",
            "inventions     invent         inv            invention      \n",
            "come           come           com            come           \n",
            "technology     technolog      technolog      technology     \n",
            "also           also           also           also           \n",
            "makes          make           mak            make           \n",
            "us             us             us             u              \n",
            "think          think          think          think          \n",
            "use            use            us             use            \n",
            "wisely         wise           wis            wisely         \n",
            "’              ’              ’              ’              \n",
            "exciting       excit          excit          exciting       \n",
            "see            see            see            see            \n",
            "’              ’              ’              ’              \n",
            "coming         come           com            coming         \n",
            "next           next           next           next           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from QuesƟon 1.\n",
        "2. Use regular expressions to:\n",
        "a. Extract all words with more than 5 leƩers.\n",
        "b. Extract all numbers (if any exist in their text).\n",
        "c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "a. Split the text into words containing only alphabets (removing digits and special\n",
        "characters).     \n",
        "b. Extract words starƟng with a vowel."
      ],
      "metadata": {
        "id": "h2TNK_uLAyLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Original paragraph\n",
        "paragraph = \"\"\"I really like technology because it changes how we live and work.\n",
        "               New things like smartwatches, robots, and AI make life easier and more fun.\n",
        "               It helps people talk and share ideas from anywhere in the world.\n",
        "               I enjoy seeing how fast new inventions come out.\n",
        "               Technology also makes us think about how we use it wisely.\n",
        "               It’s exciting to see what’s coming next.\"\"\"\n",
        "\n",
        "# a. Extract all words with more than 5 letters\n",
        "long_words = re.findall(r'\\b[a-zA-Z]{6,}\\b', paragraph)\n",
        "print(\"Words with more than 5 letters:\\n\", long_words)\n",
        "\n",
        "# b. Extract all numbers\n",
        "numbers = re.findall(r'\\b\\d+\\b', paragraph)\n",
        "print(\"\\nNumbers found:\\n\", numbers)\n",
        "\n",
        "# c. Extract all capitalized words\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', paragraph)\n",
        "print(\"\\nCapitalized words:\\n\", capitalized_words)\n",
        "\n",
        "# --- Text Splitting Techniques ---\n",
        "\n",
        "# a. Split text into words (only alphabets), removing digits and special characters\n",
        "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', paragraph)\n",
        "print(\"\\nAlphabet-only words:\\n\", alphabetic_words)\n",
        "\n",
        "# b. Extract words starting with a vowel (case-insensitive)\n",
        "vowel_words = [word for word in alphabetic_words if re.match(r'^[aeiouAEIOU]', word)]\n",
        "print(\"\\nWords starting with a vowel:\\n\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtJFVhZ6BFXz",
        "outputId": "5c5d2538-d504-40ec-a307-d8d25ccbc7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words with more than 5 letters:\n",
            " ['really', 'technology', 'because', 'changes', 'things', 'smartwatches', 'robots', 'easier', 'people', 'anywhere', 'seeing', 'inventions', 'Technology', 'wisely', 'exciting', 'coming']\n",
            "\n",
            "Numbers found:\n",
            " []\n",
            "\n",
            "Capitalized words:\n",
            " ['I', 'New', 'It', 'I', 'Technology', 'It']\n",
            "\n",
            "Alphabet-only words:\n",
            " ['I', 'really', 'like', 'technology', 'because', 'it', 'changes', 'how', 'we', 'live', 'and', 'work', 'New', 'things', 'like', 'smartwatches', 'robots', 'and', 'AI', 'make', 'life', 'easier', 'and', 'more', 'fun', 'It', 'helps', 'people', 'talk', 'and', 'share', 'ideas', 'from', 'anywhere', 'in', 'the', 'world', 'I', 'enjoy', 'seeing', 'how', 'fast', 'new', 'inventions', 'come', 'out', 'Technology', 'also', 'makes', 'us', 'think', 'about', 'how', 'we', 'use', 'it', 'wisely', 'It', 's', 'exciting', 'to', 'see', 'what', 's', 'coming', 'next']\n",
            "\n",
            "Words starting with a vowel:\n",
            " ['I', 'it', 'and', 'and', 'AI', 'easier', 'and', 'It', 'and', 'ideas', 'anywhere', 'in', 'I', 'enjoy', 'inventions', 'out', 'also', 'us', 'about', 'use', 'it', 'It', 'exciting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "1. Take original text from QuesƟon 1.\n",
        "2. Write a custom tokenizaƟon funcƟon that:\n",
        "\n",
        "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\n",
        "\"isn't\" should not be split into \"is\" and \"n't\").\n",
        "\n",
        " b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
        "a single token).\n",
        "\n",
        "  c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
        "should remain as is).\n",
        "\n",
        "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
        "a. Replace email addresses with '<EMAIL>' placeholder.\n",
        "b. Replace URLs with '<URL>' placeholder.\n",
        "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
        "'<PHONE>' placeholder."
      ],
      "metadata": {
        "id": "751gRKurBWSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Hey! I just spoke with Dr. Alan regarding the state-of-the-art AI system we're developing.\n",
        "It isn't ready yet, but we're 90.5% there—just a few bugs left.\n",
        "You can check the initial version at https://example-ai-project.dev or drop an email to alan.turing@example.com.\n",
        "Also, call him on +91 9876543210 if it's urgent.\n",
        "Don't forget to verify the budget sheet showing expenses like 3.14k on hardware and 500.75 on software tools.\n",
        "Meanwhile, Sarah's number—123-456-7890—is also available for internal coordination.\n",
        "She's handling real-time updates and last-minute testing.\n",
        "Honestly, I can't believe how fast this project has evolved—it's truly next-gen!\n",
        "\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
        "    text = re.sub(r'https?://\\S+', '<URL>', text)\n",
        "    text = re.sub(r'\\+?\\d{1,3}[ -]?\\d{10}', '<PHONE>', text)\n",
        "    text = re.sub(r'\\b\\d{3}-\\d{3}-\\d{4}\\b', '<PHONE>', text)\n",
        "    return text\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    pattern = r\"\"\"\n",
        "        \\b\\w+(?:-\\w+)+\\b\n",
        "        | \\b\\d+\\.\\d+\\b\n",
        "        | \\b\\w+'\\w+\\b\n",
        "        | \\b\\w+\\b\n",
        "        | [<>\\w]+\n",
        "    \"\"\"\n",
        "    return re.findall(pattern, text, re.VERBOSE)\n",
        "\n",
        "cleaned_text = clean_text(text)\n",
        "tokens = custom_tokenizer(cleaned_text)\n",
        "\n",
        "print(\"Cleaned Text:\\n\", cleaned_text)\n",
        "print(\"\\nTokens:\\n\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoWCduruv0Xq",
        "outputId": "daff7d24-f0d3-47a4-e3d0-0104be7aea39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            " \n",
            "Hey! I just spoke with Dr. Alan regarding the state-of-the-art AI system we're developing.\n",
            "It isn't ready yet, but we're 90.5% there—just a few bugs left.\n",
            "You can check the initial version at <URL> or drop an email to <EMAIL>.\n",
            "Also, call him on <PHONE> if it's urgent.\n",
            "Don't forget to verify the budget sheet showing expenses like 3.14k on hardware and 500.75 on software tools.\n",
            "Meanwhile, Sarah's number—<PHONE>—is also available for internal coordination.\n",
            "She's handling real-time updates and last-minute testing.\n",
            "Honestly, I can't believe how fast this project has evolved—it's truly next-gen!\n",
            "\n",
            "\n",
            "Tokens:\n",
            " ['Hey', 'I', 'just', 'spoke', 'with', 'Dr', 'Alan', 'regarding', 'the', 'state-of-the-art', 'AI', 'system', \"we're\", 'developing', 'It', \"isn't\", 'ready', 'yet', 'but', \"we're\", '90.5', 'there', 'just', 'a', 'few', 'bugs', 'left', 'You', 'can', 'check', 'the', 'initial', 'version', 'at', '<URL>', 'or', 'drop', 'an', 'email', 'to', '<EMAIL>', 'Also', 'call', 'him', 'on', '<PHONE>', 'if', \"it's\", 'urgent', \"Don't\", 'forget', 'to', 'verify', 'the', 'budget', 'sheet', 'showing', 'expenses', 'like', '3', '14k', 'on', 'hardware', 'and', '500.75', 'on', 'software', 'tools', 'Meanwhile', \"Sarah's\", 'number', '<PHONE>', 'is', 'also', 'available', 'for', 'internal', 'coordination', \"She's\", 'handling', 'real-time', 'updates', 'and', 'last-minute', 'testing', 'Honestly', 'I', \"can't\", 'believe', 'how', 'fast', 'this', 'project', 'has', 'evolved', \"it's\", 'truly', 'next-gen']\n"
          ]
        }
      ]
    }
  ]
}